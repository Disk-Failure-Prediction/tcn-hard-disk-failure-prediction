{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import wget\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from algorithms.Networks_pytorch import *\n",
    "from algorithms.Dataset_manipulation import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: still a lot of work to do here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Make sure to change these configs before running the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#model = 'ST4000DM000'\n",
    "# here you can select the model. This is the one tested.\n",
    "model = 'ST3000DM001'\n",
    "#years = ['2016', '2017', '2018']\n",
    "years = ['2014', '2015', '2016', '2017', '2018']\n",
    "# many parameters that could be changed, both for unbalancing, for networks and for features.\n",
    "enable_windowing = True\n",
    "min_days_HDD = 115\n",
    "# TODO: Can be adjusted by dynamic parameters\n",
    "days_considered_as_failure = 7\n",
    "test_train_perc = 0.3\n",
    "# type of oversampling\n",
    "oversample_undersample = 2\n",
    "# balancing factor (major/minor = balancing_normal_failed)\n",
    "# TODO: We can calculate the imbalance ratio of the dataset and use this ratio to adjust the balancing factor.\n",
    "balancing_normal_failed = 20\n",
    "kernel_size = 32\n",
    "# type of classifier\n",
    "classifier = 'LSTM'\n",
    "# if you extract features for RF for example. Not tested\n",
    "perform_features_extraction = False\n",
    "CUDA_DEV = \"0\"\n",
    "# if automatically select best features\n",
    "ranking = 'Ok'\n",
    "num_features = 18\n",
    "overlap = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Train and Inference\n",
    "\n",
    "The main function of the code is to perform hard disk failure prediction using various classification algorithms. The process involves the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Start]\n",
    "    B{Step 1: Load Dataset}\n",
    "    C[Step 1.1: Import Data]\n",
    "    D[Step 1.2: Filter Out Bad HDs]\n",
    "    E[Step 1.3: Define RUL Piecewise]\n",
    "    F[Step 1.4: Subflowchart: <br>Feature Selection]\n",
    "    G[Step 1.5: Subflowchart: <br>Partition Dataset]\n",
    "    H{Classifier Selection}\n",
    "    I[RandomForest]\n",
    "    J[Subflowchart: <br>TCN]\n",
    "    K[Subflowchart: <br>LSTM]\n",
    "    L[Feature Extraction]\n",
    "    M[Reshape Data]\n",
    "    N{Subflowchart: <br>Perform Classification}\n",
    "    O[End]\n",
    "    A --> B\n",
    "    B -- Fail --> C\n",
    "    C --> D\n",
    "    D --> E\n",
    "    E --> F\n",
    "    F --> G\n",
    "    B -- Success --> G\n",
    "    G --> H\n",
    "    H --> I\n",
    "    H --> J\n",
    "    H --> K\n",
    "    I --> N\n",
    "    J --> N\n",
    "    K --> N\n",
    "    N --> O\n",
    "    L -- If perform_features_extraction is True --> M\n",
    "    M --> N\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Step 1: Load the dataset from pkl file.\n",
    "    df = pd.read_pickle(os.path.join('.', '..', 'output', f'{model}_Dataset_windowed_{history_signal}_rank_{ranking}_{num_features}_overlap_{overlap}.pkl'))\n",
    "except:\n",
    "    # Step 1.1: Import the dataset from the raw data.\n",
    "    if ranking == 'None':\n",
    "        df = import_data(years=years, model=model, name='iSTEP', features=features)\n",
    "    else:\n",
    "        df = import_data(years=years, model=model, name='iSTEP')\n",
    "    df.set_index(['serial_number', 'date'], inplace=True)\n",
    "    print(\"DF index name:\", df.index.names)\n",
    "    print(df.head())\n",
    "    for column in list(df):\n",
    "        missing = round(df[column].notna().sum() / df.shape[0] * 100, 2)\n",
    "        print('{:.<27}{}%'.format(column, missing))\n",
    "    # drop bad HDs\n",
    "    # Step 1.2: Filter out the bad HDDs.\n",
    "    bad_missing_hds, bad_power_hds, df = filter_HDs_out(df, min_days=min_days_HDD, time_window='30D', tolerance=30)\n",
    "    # predict_val represents the prediction value of the failure\n",
    "    # validate_val represents the validation value of the failure\n",
    "    # Step 1.3: Define RUL(Remain useful life) Piecewise\n",
    "    df['predict_val'], df['validate_val'] = generate_failure_predictions(df, days=days_considered_as_failure, window=history_signal)\n",
    "    if ranking != 'None':\n",
    "        # Step 1.4: Feature Selection: Subflow chart of Main Classification Process\n",
    "        df = feature_selection(df, num_features)\n",
    "    print('Used features')\n",
    "    for column in list(df):\n",
    "        print('{:.<27}'.format(column,))\n",
    "    # print('Saving to pickle file...')\n",
    "    #df.to_pickle(os.path.join(script_dir, '..', 'output', f'{model}_Dataset_windowed_{history_signal}_rank_{ranking}_{num_features}_overlap_{overlap}.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5:  Partition Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the dataset into training and testing sets.\n",
    "\n",
    "\n",
    "```mermaid\n",
    "\n",
    "graph TD\n",
    "    A[Start: dataset_partitioning] --> B[Step 1.1: Reset Index and Step 1.2: Preprocess Data]\n",
    "    B --> C{Step 2: Check Windowing}\n",
    "    C -- Yes --> D[Attempt to Load Pre-existing Windowed Dataset]\n",
    "    D -- Success --> E[Loaded Existing Dataset]\n",
    "    E --> F[Prepare Data for Modeling]\n",
    "    D -- Failure --> G[Windowing Process]\n",
    "    G --> F\n",
    "    C -- No --> F\n",
    "    F --> H{Technique Selection}\n",
    "    H -- Random --> I[Random Partitioning]\n",
    "    H -- HDD --> J[HDD Partitioning]\n",
    "    H -- Other --> K[Other Technique]\n",
    "    I --> L[Apply Sampling Techniques]\n",
    "    J --> L\n",
    "    K --> L\n",
    "    L --> M[Final Dataset Creation]\n",
    "    M --> N[Return Train and Test Sets]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technique='random'\n",
    "window_dim=kernel_size\n",
    "resampler_balancing=balancing_normal_failed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "\"DF index name:\", df.index.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```python\n",
    "> def partition(self):\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler(feature_range=(0, 1)) # Normalize the dataset\n",
    "\n",
    "# Extract temporal data\n",
    "# Updated: temporal now also drops 'model' and 'capacity_bytes' columns, because they are object. We need float64.\n",
    "temporal = df[['serial_number', 'date', 'failure', 'predict_val', 'validate_val', 'model', 'capacity_bytes']]\n",
    "df.drop(columns=temporal.columns, inplace=True)\n",
    "df = pd.DataFrame(mms.fit_transform(df),\n",
    "columns=df.columns, index=df.index)  # FIXME: \n",
    "df = pd.concat([df, temporal], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Check Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```python\n",
    "> def handle_windowing(self):\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2: Not checking for windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not enable_windowing:\n",
    "    windowed_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.1: Checking for windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_windowing():\n",
    "    raise NotImplementedError(\"封的真好，下次别封了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_windowing:\n",
    "    try:\n",
    "        # Step 2.1.1: If Yes, attempt to load the pre-processed windowed dataset.\n",
    "        windowed_df = pd.read_pickle(\n",
    "            os.path.join(\n",
    "                \".\",\n",
    "                \"..\",\n",
    "                \"output\",\n",
    "                f\"{model}_Dataset_windowed_{window_dim}_rank_{rank}_{num_features}_overlap_{overlap}.pkl\",\n",
    "            )\n",
    "        )\n",
    "        print(\"Loading the windowed dataset\")\n",
    "        windowed_df\n",
    "        # straight to Step 3\n",
    "        # return rename_columns(windowed_df)\n",
    "    except FileNotFoundError:\n",
    "        # Step 2.1.2: If No, perform windowing on the dataset.\n",
    "        print(\"Windowing the df\")  # FIXME: Currently all columns are indexed.\n",
    "        # From now on, `def perform_windowing(self):`\n",
    "        # TODO: put perform_windowing here\n",
    "        windowed_df = perform_windowing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```python\n",
    "> def rename_columns(self, df):\n",
    "> ```\n",
    "Note: In `Dataset_manipulation.py`, the `df` here should be `windowed_df`, returns to and ultimately assigned back to `windowed_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "count = {}\n",
    "print(\"\\nTEST\", windowed_df.columns)\n",
    "for column in windowed_df.columns:\n",
    "    if column not in count:\n",
    "        count[column] = 0\n",
    "    count[column] += 1\n",
    "    new_column = f\"{column}_{count[column]}\" if count[column] > 1 else column\n",
    "    cols.append(new_column)\n",
    "windowed_df.columns = cols\n",
    "windowed_df.sort_index(axis=1, inplace=True)\n",
    "\n",
    "print(\"\\nTest: \", count[\"predict_val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Technique Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```python\n",
    "> def random_split(self, df):\n",
    "> ```\n",
    "\n",
    "> ```python\n",
    "> def preprocess_random(self, df):\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if technique == \"random\":\n",
    "    # if self.windowing == 1:\n",
    "    #     X = self.arrays_to_matrix(X)\n",
    "    # TODO: 老母猪戴胸罩，一套又一套！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(X_train, Y_train, X_test, Y_test, classifier, metric, **args):\n",
    "    \"\"\"\n",
    "    Perform classification using the specified classifier.\n",
    "    --- Step 1.7: Perform Classification\n",
    "    Parameters:\n",
    "    - X_train (array-like): Training data features.\n",
    "    - Y_train (array-like): Training data labels.\n",
    "    - X_test (array-like): Test data features.\n",
    "    - Y_test (array-like): Test data labels.\n",
    "    - classifier (str): The classifier to use. Options: 'RandomForest', 'TCN', 'LSTM'.\n",
    "    - metric (str): The metric to evaluate the classification performance.\n",
    "    - **args: Additional arguments specific to each classifier.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print('Classification using {} is starting'.format(classifier))\n",
    "    Y_test_real = []\n",
    "    prediction = []\n",
    "    if classifier == 'RandomForest':\n",
    "        # Step 1.7.1: Perform Classification using RandomForest: Use RandomForest Libaray. Train and validate the network using RandomForest.\n",
    "        X_train, Y_train = shuffle(X_train, Y_train)\n",
    "        # Use third-party RandomForest library.\n",
    "        model = RandomForestClassifier(n_estimators=30, min_samples_split=10, random_state=3)\n",
    "        model.fit(X_train[:, :], Y_train)\n",
    "        prediction = model.predict(X_test)\n",
    "        Y_test_real = Y_test\n",
    "        report_metrics(Y_test_real, prediction, metric)\n",
    "    elif classifier == 'TCN':\n",
    "        # Step 1.7.2: Perform Classification using TCN. Subflowchart: TCN Subflowchart. Train and validate the network using TCN\n",
    "        net_train_validate_TCN(args['net'], args['optimizer'], X_train, Y_train, X_test, Y_test, args['epochs'], args['batch_size'], args['lr'])\n",
    "    elif classifier == 'LSTM':\n",
    "        # Step 1.7.3: Perform Classification using LSTM. Subflowchart: LSTM Subflowchart. Train and validate the network using LSTM\n",
    "        train_dataset = FPLSTMDataset(X_train, Y_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, collate_fn=FPLSTM_collate)\n",
    "        test_dataset = FPLSTMDataset(X_test, Y_test.values)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=True, collate_fn=FPLSTM_collate)\n",
    "        net_train_validate_LSTM(args['net'], args['optimizer'], train_loader, test_loader, args['epochs'], X_test.shape[0], Xtrain.shape[0], args['lr'])\n",
    "        pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
